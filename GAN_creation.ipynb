{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone impala mk2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_device = tf.config.list_physical_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(gpu_device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use gpu while training use this code snippit\n",
    "```\n",
    "with tf.device('/GPU:0'):\n",
    "    # Create your TensorFlow model here\n",
    "    model = tf.keras.Sequential([...])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the lyrics dataframe\n",
    "df = pd.read_csv(\"corpuses\\\\tame impala lyrics.csv\")\n",
    "df = df.dropna()\n",
    "\n",
    "# Joining all lines of lyrics into one single list\n",
    "lyrics_corpus = []\n",
    "for song_lryics in df['lyrics']:\n",
    "    # Splitting the lyrics by line\n",
    "    song_lryics = song_lryics.splitlines()\n",
    "    \n",
    "    #Removing empty lines\n",
    "    song_lryics = list(filter(None, song_lryics))\n",
    "\n",
    "    # Joining back lines\n",
    "    song_lryics = '\\n'.join(song_lryics)\n",
    "\n",
    "    # Appending to songs corpus\n",
    "    lyrics_corpus.append(song_lryics)\n",
    "\n",
    "lyrics_corpus = '\\n'.join(lyrics_corpus)\n",
    "lyrics_corpus = lyrics_corpus.split('\\n')\n",
    "lyrics_corpus = [line + ' \\n' for line in lyrics_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 2000\n"
     ]
    }
   ],
   "source": [
    "#To Load the tokenizer\n",
    "with open('models\\\\tame_impala_tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "    total_words = tokenizer.num_words\n",
    "    print(f\"Total words: {total_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences length (of each sequence): 200\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the text file\n",
    "input_sequences = np.loadtxt('corpuses\\\\input_sequences.txt', dtype=int)\n",
    "\n",
    "# Reshape the loaded array to its original shape\n",
    "original_shape = (18466, 200, 1)\n",
    "input_sequences = input_sequences.reshape(original_shape)\n",
    "\n",
    "sequence_len = len(input_sequences[0])\n",
    "print(f\"Input sequences length (of each sequence): {sequence_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating a sequence as input to generator\n",
    "def generate_random_sequence():\n",
    "    # Picking random length of lyric\n",
    "    random_lyric_size = np.random.randint(1,200)\n",
    "    \n",
    "    # Generating till random length\n",
    "    lyrics = [np.random.randint(0, 2000) for x in range(random_lyric_size)]\n",
    "\n",
    "    # Padding with 0s\n",
    "    padded_lyrics = [0 for _ in range(200 - random_lyric_size)]\n",
    "    padded_lyrics.extend(lyrics)\n",
    "    \n",
    "    return np.array([padded_lyrics]).reshape(200,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genius api auth keys\n",
    "file = open(\"genius_auth.json\")\n",
    "genius_auth = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lyricsgenius import Genius\n",
    "\n",
    "# API Client\n",
    "genius = Genius(genius_auth['client_access_token'],\n",
    "                verbose=False,\n",
    "                skip_non_songs=True, \n",
    "                excluded_terms=[\"(Remix)\", \"(Live)\"], \n",
    "                remove_section_headers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the songs\n",
    "songs = genius.search_artist(artist_name=\"Tame Impala\", max_songs=None).songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...),\n",
       " Song(id, artist, ...)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning the obtained information into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'159 ContributorsTranslationsFrançaisTürkçeEspañolPortuguêsItalianoDeutschThe Less I Know the Better Lyrics\\nSomeone said they left together\\nI ran out the door to get her\\nShe was holding hands with Trevor\\nNot the greatest feeling ever\\nSaid, \"Pull yourself together\\nYou should try your luck with Heather\"\\nThen I heard they slept together\\nOh, the less I know the better\\nThe less I know the better\\n\\nOh, my love, can\\'t you see yourself by my side?\\nNo surprise, when you\\'re on his shoulder like every night\\nOh, my love, can\\'t you see that you\\'re on my mind?\\nDon\\'t suppose you could convince your lover to change his mind\\nSo goodbye\\n\\nShe said, \"It\\'s not now or never\\nWait ten years, we\\'ll be together\"\\nI said, \"Better late than never\\nJust don\\'t make me wait forever\"\\nDon\\'t make me wait forever\\nDon\\'t make me wait forever\\nYou might also like\\nOh, my love, can\\'t you see yourself by my side?\\nI don\\'t suppose you could convince your lover to change his mind\\n\\nI was doing fine without ya\\n\\'Til I saw your face, now I can\\'t erase\\nGivin\\' in to all his bullshit\\nIs this what you want? Is this who you are?\\nI was doing fine without ya\\n\\'Til I saw your eyes turn away from mine\\nOh, sweet darling, where he wants you\\nSaid, \"Come on, Superman, say your stupid line\"\\nSaid, \"Come on, Superman, say your stupid line\"\\nSaid, \"Come on, Superman, say your stupid line\"206Embed'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs[0].lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"id\":[],\n",
    "    \"artist\":[],\n",
    "    \"title\":[],\n",
    "    \"lyrics\":[]\n",
    "}\n",
    "\n",
    "# Iterating through the list\n",
    "for song in songs:\n",
    "    if not song:\n",
    "        continue\n",
    "    data[\"id\"].append(song.id)\n",
    "    data[\"artist\"].append(song.artist)\n",
    "    data[\"title\"].append(song.title)\n",
    "\n",
    "    # Preprocessing the lyrics to remove watermarks\n",
    "    song_lyrics = song.lyrics\n",
    "    ## Getting rid of the first line (shows song metadata)\n",
    "    song_lyrics = re.sub(r'^.*?Lyrics','',song_lyrics)\n",
    "    ## Removing the watermark in the last line\n",
    "    song_lyrics = re.sub(r'(You might also like)?(\\d*)?Embed','',song_lyrics, flags=re.IGNORECASE)\n",
    "    ## Splitting the lyrics by line\n",
    "    song_lyrics = song_lyrics.splitlines()\n",
    "    ##Removing empty lines\n",
    "    song_lyrics = list(filter(None, song_lyrics))\n",
    "    ## Joining back lines\n",
    "    song_lyrics = '\\n'.join(song_lyrics)\n",
    "\n",
    "    data[\"lyrics\"].append(song_lyrics)\n",
    "    \n",
    "\n",
    "# Turning into dataframe\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('corpuses\\\\tame impala lyrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2165830</td>\n",
       "      <td>Tame Impala</td>\n",
       "      <td>The Less I Know the Better</td>\n",
       "      <td>Someone said they left together\\nI ran out the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2165813</td>\n",
       "      <td>Tame Impala</td>\n",
       "      <td>New Person, Same Old Mistakes</td>\n",
       "      <td>I can just hear them now\\n\"How could you let u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>721026</td>\n",
       "      <td>Tame Impala</td>\n",
       "      <td>Let It Happen</td>\n",
       "      <td>It's always around me, all this noise\\nBut not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2165828</td>\n",
       "      <td>Tame Impala</td>\n",
       "      <td>Yes I’m Changing</td>\n",
       "      <td>I was raging, it was late\\nIn the world my dem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94120</td>\n",
       "      <td>Tame Impala</td>\n",
       "      <td>Feels Like We Only Go Backwards</td>\n",
       "      <td>It feels like I only go backwards, baby\\nEvery...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       artist                            title  \\\n",
       "0  2165830  Tame Impala       The Less I Know the Better   \n",
       "1  2165813  Tame Impala    New Person, Same Old Mistakes   \n",
       "2   721026  Tame Impala                    Let It Happen   \n",
       "3  2165828  Tame Impala                 Yes I’m Changing   \n",
       "4    94120  Tame Impala  Feels Like We Only Go Backwards   \n",
       "\n",
       "                                              lyrics  \n",
       "0  Someone said they left together\\nI ran out the...  \n",
       "1  I can just hear them now\\n\"How could you let u...  \n",
       "2  It's always around me, all this noise\\nBut not...  \n",
       "3  I was raging, it was late\\nIn the world my dem...  \n",
       "4  It feels like I only go backwards, baby\\nEvery...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"corpuses\\\\tame impala lyrics.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining all lines of lyrics into one single list\n",
    "lyrics_corpus = []\n",
    "for song_lryics in df['lyrics']:\n",
    "    # Splitting the lyrics by line\n",
    "    song_lryics = song_lryics.splitlines()\n",
    "    \n",
    "    #Removing empty lines\n",
    "    song_lryics = list(filter(None, song_lryics))\n",
    "\n",
    "    # Joining back lines\n",
    "    song_lryics = '\\n'.join(song_lryics)\n",
    "\n",
    "    # Appending to songs corpus\n",
    "    lyrics_corpus.append(song_lryics)\n",
    "\n",
    "lyrics_corpus = '\\n'.join(lyrics_corpus)\n",
    "lyrics_corpus = lyrics_corpus.split('\\n')\n",
    "lyrics_corpus = [line + ' \\n' for line in lyrics_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Someone said they left together \\n',\n",
       " 'I ran out the door to get her \\n',\n",
       " 'She was holding hands with Trevor \\n']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in tokenizer:2000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Making tokenizer\n",
    "tokenizer = Tokenizer(num_words=2000, oov_token=\"OOV\",\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t')\n",
    "\n",
    "# Fitting corpus\n",
    "tokenizer.fit_on_texts(lyrics_corpus)\n",
    "\n",
    "\n",
    "total_words = tokenizer.num_words\n",
    "print(f\"Total words in tokenizer:{total_words}\")\n",
    "\n",
    "# Saving tokenizer\n",
    "with open('models\\\\tame_impala_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Load the tokenizer\n",
    "with open('models\\\\tame_impala_tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "    total_words = tokenizer.num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the corpus into sequences\n",
    "sequences = []\n",
    "for line in lyrics_corpus:\n",
    "    tokenized_line = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(tokenized_line)):\n",
    "        n_gram_sequence = tokenized_line[:i]\n",
    "        sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences\n",
    "max_sequence_len = 200\n",
    "num_sequences = len(sequences)\n",
    "input_sequences = np.array(pad_sequences(sequences, \n",
    "                                   maxlen=max_sequence_len,\n",
    "                                   padding = 'pre')).reshape(num_sequences, 200, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the input sequences\n",
    "filename = \"corpuses\\\\input_sequences.txt\"\n",
    "\n",
    "# Use numpy.savetxt to save the array to a text file\n",
    "np.savetxt(filename, input_sequences.reshape(-1, input_sequences.shape[-1]), fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dropout, Bidirectional, LSTM, Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # This model is very similar to the model used in 'clone impala mk1'\n",
    "    # model.add(Embedding(input_dim=total_words, output_dim=64, \n",
    "    #                     input_length=sequence_len))\n",
    "    model.add(Dense(8*8*256, input_dim=256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    # model.add(Dense(200, activation='relu'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(64, input_dim=64))\n",
    "    model.add(Reshape())\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add(Dense(200))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"bidirectional_6\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\GAN_creation.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sanya/Documents/Code%20projects/clone-impala-mk2/GAN_creation.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m generator \u001b[39m=\u001b[39m build_generator()\n",
      "\u001b[1;32mc:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\GAN_creation.ipynb Cell 35\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanya/Documents/Code%20projects/clone-impala-mk2/GAN_creation.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m Sequential()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanya/Documents/Code%20projects/clone-impala-mk2/GAN_creation.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39madd(Dense(\u001b[39m64\u001b[39m, input_dim\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sanya/Documents/Code%20projects/clone-impala-mk2/GAN_creation.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39;49madd(Bidirectional(LSTM(\u001b[39m64\u001b[39;49m, return_sequences\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanya/Documents/Code%20projects/clone-impala-mk2/GAN_creation.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39madd(Bidirectional(LSTM(\u001b[39m64\u001b[39m)))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanya/Documents/Code%20projects/clone-impala-mk2/GAN_creation.ipynb#X46sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39madd(Dense(\u001b[39m200\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\env\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\env\\lib\\site-packages\\keras\\engine\\input_spec.py:232\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    230\u001b[0m     ndim \u001b[39m=\u001b[39m shape\u001b[39m.\u001b[39mrank\n\u001b[0;32m    231\u001b[0m     \u001b[39mif\u001b[39;00m ndim \u001b[39m!=\u001b[39m spec\u001b[39m.\u001b[39mndim:\n\u001b[1;32m--> 232\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    233\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    234\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m, found ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mmax_ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"bidirectional_6\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 64)"
     ]
    }
   ],
   "source": [
    "generator = build_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 200) for input KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name='embedding_2_input'), name='embedding_2_input', description=\"created by layer 'embedding_2_input'\"), but it was called on an input with incompatible shape (32, 1).\n",
      "8/8 [==============================] - 13s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.4941040e-03,  1.1103499e-03,  8.2597136e-04, ...,\n",
       "        -1.0661436e-04, -1.8723702e-04, -7.1735657e-04],\n",
       "       [ 1.4941040e-03,  1.1103499e-03,  8.2597136e-04, ...,\n",
       "        -1.0661436e-04, -1.8723702e-04, -7.1735657e-04],\n",
       "       [ 1.4941040e-03,  1.1103499e-03,  8.2597136e-04, ...,\n",
       "        -1.0661436e-04, -1.8723702e-04, -7.1735657e-04],\n",
       "       ...,\n",
       "       [-9.9160956e-05,  1.4566074e-03,  1.5361075e-03, ...,\n",
       "         1.4107404e-03, -2.9172984e-04, -4.2169951e-05],\n",
       "       [ 1.4941040e-03,  1.1103499e-03,  8.2597136e-04, ...,\n",
       "        -1.0661436e-04, -1.8723702e-04, -7.1735657e-04],\n",
       "       [ 1.4941040e-03,  1.1103499e-03,  8.2597136e-04, ...,\n",
       "        -1.0661436e-04, -1.8723702e-04, -7.1735657e-04]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.predict(tf.random.normal((256, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 200, 64)           128000    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 200, 64)           0         \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 200, 128)         66048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 200)               25800     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 318,664\n",
      "Trainable params: 318,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strictly speaking insane loss loss loss loss                    \n"
     ]
    }
   ],
   "source": [
    "# Generating some new lyrics\n",
    "next_words = 25\n",
    "seed_text = \"Strictly speaking\"\n",
    "for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=sequence_len, padding='pre')\n",
    "        predicted = np.argmax(generator.predict(token_list, verbose=0), axis=-1)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The untrained model produces gibbrish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator will evaluate the probabilities for the next word produced by the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Conv1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    # First Conv Block\n",
    "    model.add(Conv1D(32, 5, input_shape = (2000,1)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # Second Conv Block\n",
    "    model.add(Conv1D(64, 5))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # Third Conv Block\n",
    "    model.add(Conv1D(128, 5))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # Fourth Conv Block\n",
    "    model.add(Conv1D(256, 5))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # Flatten then pass to dense layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 1996, 32)          192       \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 1996, 32)          0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1996, 32)          0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 1992, 64)          10304     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 1992, 64)          0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1992, 64)          0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 1988, 128)         41088     \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 1988, 128)         0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1988, 128)         0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 1984, 256)         164096    \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 1984, 256)         0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1984, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 507904)            0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 507904)            0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 507905    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 723,585\n",
      "Trainable params: 723,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = build_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00050096, 0.00050081, 0.00050014, ..., 0.00049978, 0.00050044,\n",
       "        0.00049993]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [[np.random.randint(0,2000) for x in range (200)]]\n",
    "probs = generator.predict(tokens)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.50000167]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator.predict(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the untrained discriminator is unsure weather this is a real lyric or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up losses and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer for both\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Binary cross entropy for both\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_opt = Adam(learning_rate=.0001)\n",
    "d_opt = Adam(learning_rate=.00001)\n",
    "\n",
    "g_loss = BinaryCrossentropy()\n",
    "d_loss = BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building subclass model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsGAN(Model):\n",
    "    def __init__(self, generator, discriminator, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def compile(self, g_opt, d_opt, g_loss, d_loss,  *args, **kwargs):\n",
    "        super().compile( *args, **kwargs)\n",
    "\n",
    "        self.g_opt = g_opt\n",
    "        self.d_opt = d_opt\n",
    "        self.g_loss = g_loss\n",
    "        self.d_loss = d_loss\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        # Get the data \n",
    "        real_lyrics = batch\n",
    "        fake_lyrics = self.generator(generate_random_sequence(), \n",
    "                                     training=False)\n",
    "        \n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as d_tape: \n",
    "            # Pass the real and fake images to the discriminator model\n",
    "            yhat_real = self.discriminator(real_lyrics, training=True) \n",
    "            yhat_fake = self.discriminator(fake_lyrics, training=True)\n",
    "            yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)\n",
    "            \n",
    "            # Create labels for real and fakes images\n",
    "            y_realfake = tf.concat([tf.zeros_like(yhat_real), \n",
    "                                    tf.ones_like(yhat_fake)], axis=0)\n",
    "            \n",
    "            # Add some noise to the TRUE outputs\n",
    "            noise_real = 0.15*tf.random.uniform(tf.shape(yhat_real))\n",
    "            noise_fake = -0.15*tf.random.uniform(tf.shape(yhat_fake))\n",
    "            y_realfake += tf.concat([noise_real, noise_fake], axis=0)\n",
    "            \n",
    "            # Calculate loss - BINARYCROSS \n",
    "            total_d_loss = self.d_loss(y_realfake, yhat_realfake)\n",
    "            \n",
    "        # Apply backpropagation - nn learn \n",
    "        dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables) \n",
    "        self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))\n",
    "        \n",
    "        # Train the generator \n",
    "        with tf.GradientTape() as g_tape:\n",
    "            # Generate some new images\n",
    "            gen_lyrics = self.generator(generate_random_tokens(), \n",
    "                                        training=True)\n",
    "                                        \n",
    "            # Create the predicted labels\n",
    "            predicted_labels = self.discriminator(gen_lyrics, training=False)\n",
    "                                        \n",
    "            # Calculate loss - trick to training to fake out the discriminator\n",
    "            total_g_loss = self.g_loss(tf.zeros_like(predicted_labels), predicted_labels) \n",
    "            \n",
    "        # Apply backprop\n",
    "        ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))\n",
    "        \n",
    "        return {\"d_loss\":total_d_loss, \"g_loss\":total_g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of subclassed model\n",
    "lyricsgan = LyricsGAN(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "lyricsgan.compile(g_opt, d_opt, g_loss, d_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is only to store the generated output of the model as we are training, not necessary to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMonitor(Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.uniform((self.num_img, self.latent_dim,1))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = array_to_img(generated_images[i])\n",
    "            img.save(os.path.join('images', f'generated_img_{epoch}_{i}.png')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\sanya\\AppData\\Local\\Temp\\ipykernel_8900\\2911947785.py\", line 25, in train_step\n        yhat_real = self.discriminator(real_lyrics, training=True)\n    File \"c:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 2000, 1), found shape=(32, 34)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\GAN_creation.ipynb Cell 60\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanya/Documents/Code%20projects/clone-impala-mk2/GAN_creation.ipynb#Y113sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Recommend 2000 epochs\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sanya/Documents/Code%20projects/clone-impala-mk2/GAN_creation.ipynb#Y113sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# hist = lyricsgan.fit(ds, epochs=20, callbacks=[ModelMonitor()])\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sanya/Documents/Code%20projects/clone-impala-mk2/GAN_creation.ipynb#Y113sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m hist \u001b[39m=\u001b[39m lyricsgan\u001b[39m.\u001b[39;49mfit(input_sequences, epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filee7l4celv.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\GAN_creation.ipynb Cell 60\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanya/Documents/Code%20projects/clone-impala-mk2/GAN_creation.ipynb#Y113sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Train the discriminator\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanya/Documents/Code%20projects/clone-impala-mk2/GAN_creation.ipynb#Y113sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m d_tape: \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanya/Documents/Code%20projects/clone-impala-mk2/GAN_creation.ipynb#Y113sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# Pass the real and fake images to the discriminator model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sanya/Documents/Code%20projects/clone-impala-mk2/GAN_creation.ipynb#Y113sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     yhat_real \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdiscriminator(real_lyrics, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanya/Documents/Code%20projects/clone-impala-mk2/GAN_creation.ipynb#Y113sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     yhat_fake \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscriminator(fake_lyrics, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sanya/Documents/Code%20projects/clone-impala-mk2/GAN_creation.ipynb#Y113sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     yhat_realfake \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconcat([yhat_real, yhat_fake], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\sanya\\AppData\\Local\\Temp\\ipykernel_8900\\2911947785.py\", line 25, in train_step\n        yhat_real = self.discriminator(real_lyrics, training=True)\n    File \"c:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\sanya\\Documents\\Code projects\\clone-impala-mk2\\env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 2000, 1), found shape=(32, 34)\n"
     ]
    }
   ],
   "source": [
    "# Recommend 2000 epochs\n",
    "# hist = lyricsgan.fit(ds, epochs=20, callbacks=[ModelMonitor()])\n",
    "hist = lyricsgan.fit(input_sequences, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.suptitle('Loss')\n",
    "plt.plot(hist.history['d_loss'], label='d_loss')\n",
    "plt.plot(hist.history['g_loss'], label='g_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
